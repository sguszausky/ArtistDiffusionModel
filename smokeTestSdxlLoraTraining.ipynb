{
 "cells": [
  {
   "cell_type": "code",
   "id": "b3707f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T17:57:08.317106Z",
     "start_time": "2026-01-28T17:57:04.666604Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from peft import LoraConfig\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import sdxl_lora_trainer_helper as helper\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.0\n",
      "cuda available: False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "322d86ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T17:57:13.573759Z",
     "start_time": "2026-01-28T17:57:13.569026Z"
    }
   },
   "source": [
    "#Using tiny model for smoke test, cpu instead of cuda and small resulation, only 1 training step\n",
    "\n",
    "MODEL_ID = os.getenv(\"SMOKE_MODEL\", \"dg845/tiny-random-stable-diffusion-xl\")\n",
    "DEVICE = os.getenv(\"SMOKE_DEVICE\", \"cuda\" if torch.cuda.is_available() else \"cpu\").lower()\n",
    "STEPS = int(os.getenv(\"SMOKE_STEPS\", \"1\"))\n",
    "RES = int(os.getenv(\"SMOKE_RES\", \"64\"))\n",
    "\n",
    "if DEVICE == \"cuda\" and not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"DEVICE=cuda requested but CUDA is not available.\")\n",
    "\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"DTYPE:\", DTYPE)\n",
    "print(\"STEPS:\", STEPS, \"RES:\", RES)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ID: dg845/tiny-random-stable-diffusion-xl\n",
      "DEVICE: cpu\n",
      "DTYPE: torch.float32\n",
      "STEPS: 1 RES: 64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6e6e1fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T17:57:25.330695Z",
     "start_time": "2026-01-28T17:57:21.290166Z"
    }
   },
   "source": [
    "set_seed(0)\n",
    "\n",
    "ds_root = Path(\"dataset/small_test\")\n",
    "\n",
    "out_root = Path(\"./smoke_out\")\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=1, mixed_precision=None)\n",
    "\n",
    "local_files_only = os.getenv(\"HF_HUB_OFFLINE\", \"\") == \"1\"\n",
    "print(\"HF_HUB_OFFLINE:\", os.getenv(\"HF_HUB_OFFLINE\", \"\"), \"=> local_files_only:\", local_files_only)\n",
    "\n",
    "print(\"Loading pipeline…\")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    use_safetensors=True,\n",
    "    local_files_only=local_files_only,\n",
    ")\n",
    "pipe = pipe.to(DEVICE)\n",
    "print(\"Pipeline loaded \")\n",
    "\n",
    "unet = pipe.unet\n",
    "vae = pipe.vae\n",
    "te1 = pipe.text_encoder\n",
    "te2 = pipe.text_encoder_2\n",
    "tok1 = pipe.tokenizer\n",
    "tok2 = pipe.tokenizer_2\n",
    "\n",
    "# Freezing base weights\n",
    "vae.requires_grad_(False)\n",
    "te1.requires_grad_(False)\n",
    "te2.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "# Add LoRA\n",
    "unet_lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    ")\n",
    "\n",
    "unet.add_adapter(unet_lora_config)\n",
    "\n",
    "lora_params = [p for n, p in unet.named_parameters() if \"lora\" in n and p.requires_grad]\n",
    "print(\"trainable LoRA params:\", sum(p.numel() for p in lora_params))\n",
    "assert len(lora_params) > 0, \"No LoRA params marked trainable.\"\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "ds = helper.ImageCaptionFolder(str(ds_root), resolution=RES, center_crop=True)\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "opt = torch.optim.AdamW(lora_params, lr=1e-4)\n",
    "\n",
    "unet, opt, dl = accelerator.prepare(unet, opt, dl)\n",
    "device_t = accelerator.device\n",
    "\n",
    "vae.to(device_t)\n",
    "te1.to(device_t)\n",
    "te2.to(device_t)\n",
    "unet.train()\n",
    "\n",
    "print(\"Running train step…\")\n",
    "global_step = 0\n",
    "for batch in dl:\n",
    "    if global_step >= STEPS:\n",
    "        break\n",
    "\n",
    "    with accelerator.accumulate(unet):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device_t, dtype=vae.dtype)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        bsz = latents.shape[0]\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device_t, dtype=torch.long\n",
    "        )\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        captions = batch[\"caption\"]\n",
    "        input_ids_1 = tok1(\n",
    "            captions, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\"\n",
    "        ).input_ids.to(device_t)\n",
    "        input_ids_2 = tok2(\n",
    "            captions, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\"\n",
    "        ).input_ids.to(device_t)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            enc1 = te1(input_ids_1, output_hidden_states=True)\n",
    "            enc2 = te2(input_ids_2, output_hidden_states=True)\n",
    "            prompt_embeds = torch.cat([enc1.hidden_states[-2], enc2.hidden_states[-2]], dim=-1)\n",
    "            pooled_prompt_embeds = enc2[0]\n",
    "\n",
    "        add_time_ids = torch.tensor(\n",
    "            [RES, RES, 0, 0, RES, RES], device=device_t, dtype=prompt_embeds.dtype\n",
    "        ).unsqueeze(0).repeat(bsz, 1)\n",
    "\n",
    "        model_pred = unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            added_cond_kwargs={\"text_embeds\": pooled_prompt_embeds, \"time_ids\": add_time_ids},\n",
    "        ).sample\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "        accelerator.backward(loss)\n",
    "        opt.step()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    print(f\"step={global_step} loss={loss.item():.6f}\")\n",
    "    global_step += 1\n",
    "\n",
    "# Testing saving + reloading LoRA weights\n",
    "ckpt = out_root / \"ckpt\"\n",
    "print(\"Saving LoRA…\")\n",
    "helper.save_unet_lora_peft(accelerator.unwrap_model(unet), str(ckpt))\n",
    "print(\"Saved \")\n",
    "\n",
    "print(\"Reload LoRA into a fresh pipeline…\")\n",
    "pipe2 = StableDiffusionXLPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_safetensors=True,\n",
    "    local_files_only=local_files_only,\n",
    ").to(DEVICE)\n",
    "\n",
    "helper.load_lora_into_pipe(pipe2, str(ckpt))\n",
    "print(\"Reload \")\n",
    "\n",
    "print(\"\\n TEST PASSED\")\n",
    "print(\"Artifacts dir:\", out_root.resolve())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HUB_OFFLINE:  => local_files_only: False\n",
      "Loading pipeline…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1f43a1d8d514616bc6b96fc9fc84645"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded \n",
      "trainable LoRA params: 8832\n",
      "Running train step…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/techfak/user/sguszausky/ArtistDiffusionModel/sdxl_lora_trainer_helper.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  arr = torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n",
      "OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 loss=1.087599\n",
      "Saving LoRA…\n",
      "Saved \n",
      "Reload LoRA into a fresh pipeline…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d43e6cfb3214aea94ab91ba1b090c35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload \n",
      "\n",
      " TEST PASSED\n",
      "Artifacts dir: /techfak/user/sguszausky/ArtistDiffusionModel/smoke_out\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1cd0e0a96bb5ff00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (StyleDiff)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
